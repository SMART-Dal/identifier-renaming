{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a4ecbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T19:54:25.610546Z",
     "start_time": "2023-08-15T19:54:19.438861Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sanid/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/sanid/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sanid/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init, MarginRankingLoss\n",
    "from transformers import BertModel, RobertaModel\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "from torch.optim import Adam\n",
    "from distutils.version import LooseVersion\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from torch.autograd import Variable\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "import nltk\n",
    "import re\n",
    "import Levenshtein\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from numpy import linalg as LA\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from nltk.corpus import wordnet\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from nltk.corpus import words as wal\n",
    "\n",
    "# import csv\n",
    "\n",
    "\n",
    "# from trl import PPOTrainer, PPOConfig, AutoModelForMaskedLM, create_reference_model\n",
    "# from trl.core import respond_to_batch\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fbaa169",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T06:53:33.239702Z",
     "start_time": "2023-08-15T06:53:33.237716Z"
    }
   },
   "outputs": [],
   "source": [
    "# java_keywords = [\n",
    "#     \"abstract\",\n",
    "#     \"assert\",\n",
    "#     \"boolean\",\n",
    "#     \"break\",\n",
    "#     \"byte\",\n",
    "#     \"case\",\n",
    "#     \"catch\",\n",
    "#     \"char\",\n",
    "#     \"class\",\n",
    "#     \"const\",\n",
    "#     \"continue\",\n",
    "#     \"default\",\n",
    "#     \"do\",\n",
    "#     \"double\",\n",
    "#     \"else\",\n",
    "#     \"enum\",\n",
    "#     \"extends\",\n",
    "#     \"exception\",\n",
    "#     \"error\",\n",
    "#     \"method\",\n",
    "#     \"builder\",\n",
    "#     \"null\",\n",
    "#     \"one\",\n",
    "#     \"two\",\n",
    "#     \"three\",\n",
    "#     \"array\",\n",
    "#     \"callback\",\n",
    "#     \"zero\",\n",
    "#     \"parameter\",\n",
    "#     \"parameters\",\n",
    "#     \"final\",\n",
    "#     \"finally\",\n",
    "#     \"float\",\n",
    "#     \"for\",\n",
    "#     \"goto\",\n",
    "#     \"if\",\n",
    "#     \"implements\",\n",
    "#     \"import\",\n",
    "#     \"instanceof\",\n",
    "#     \"int\",\n",
    "#     \"interface\",\n",
    "#     \"long\",\n",
    "#     \"native\",\n",
    "#     \"new\",\n",
    "#     \"package\",\n",
    "#     \"private\",\n",
    "#     \"protected\",\n",
    "#     \"public\",\n",
    "#     \"return\",\n",
    "#     \"short\",\n",
    "#     \"static\",\n",
    "#     \"strictfp\",\n",
    "#     \"string\",\n",
    "#     \"super\",\n",
    "#     \"switch\",\n",
    "#     \"synchronized\",\n",
    "#     \"this\",\n",
    "#     \"throw\",\n",
    "#     \"throws\",\n",
    "#     \"transient\",\n",
    "#     \"try\",\n",
    "#     \"void\",\n",
    "#     \"volatile\",\n",
    "#     \"while\"\n",
    "# ]\n",
    "# REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "# BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "# STOPWORDS =nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d4d6145",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T06:53:33.242691Z",
     "start_time": "2023-08-15T06:53:33.240618Z"
    }
   },
   "outputs": [],
   "source": [
    "def freeze(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = True\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.0\"):\n",
    "#             param.requires_grad = False\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.1\"):\n",
    "#             param.requires_grad = False\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.2\"):\n",
    "#             param.requires_grad = False\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.3\"):\n",
    "#             param.requires_grad = False\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.4\"):\n",
    "#             param.requires_grad = False\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.5\"):\n",
    "#             param.requires_grad = False\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.6\"):\n",
    "#             param.requires_grad = False\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.7\"):\n",
    "#             param.requires_grad = False\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.8\"):\n",
    "#             param.requires_grad = False\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.9\"):\n",
    "#             param.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb5e6b40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T06:53:33.246776Z",
     "start_time": "2023-08-15T06:53:33.244172Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,file_name):\n",
    "        df1 = pd.read_csv(file_name)\n",
    "        df1 = df1.fillna(\"\")\n",
    "        res = df1['X']\n",
    "#         ab = df1['X']\n",
    "#         res = [sub.replace(\"<mask>\", \"[MASK]\") for sub in ab]\n",
    "        self.X_list = res\n",
    "        self.y_list = df1['y']\n",
    "    def __len__(self):\n",
    "        return len(self.X_list)\n",
    "    def __getitem__(self,idx):\n",
    "        mapi = []\n",
    "        mapi.append(self.X_list[idx])\n",
    "        mapi.append(self.y_list[idx])\n",
    "        return mapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17faa79a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T06:53:33.270020Z",
     "start_time": "2023-08-15T06:53:33.247788Z"
    }
   },
   "outputs": [],
   "source": [
    "old_inp = []\n",
    "old_mhs = []\n",
    "class Step1_model(nn.Module):\n",
    "    def __init__(self, hidden_size=512):\n",
    "        global old_inp\n",
    "        global old_mhs\n",
    "        self.oi = old_inp\n",
    "        self.old_mhs = old_mhs\n",
    "        super(Step1_model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "#         self.model = AutoModel.from_pretrained(\"roberta-base\")\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "#         self.config = AutoConfig.from_pretrained(\"roberta-base\")\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained('microsoft/graphcodebert-base')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "        self.config = AutoConfig.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "        self.linear_layer = nn.Linear(self.model.config.vocab_size, self.model.config.vocab_size)\n",
    "\n",
    "#         self.model = AutoModelForMaskedLM.from_pretrained('bert-base-cased')\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "#         self.config = AutoConfig.from_pretrained(\"bert-base-cased\")\n",
    "        for param in self.model.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "    def foo (self,data):\n",
    "        result = []\n",
    "        if type(data) == tuple:\n",
    "            return data[1]\n",
    "        if type(data) == list:\n",
    "            for inner in data:\n",
    "                result.append(foo(inner))\n",
    "        res = []\n",
    "        for a in result[0]:\n",
    "            res.append(a[:2])\n",
    "        return res\n",
    "    def loss_func1(self, word, y):\n",
    "        if word =='NA':\n",
    "            return torch.full((1,), fill_value=100)\n",
    "        try:\n",
    "            pred_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', word)\n",
    "            target_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', y)\n",
    "            pred_tag = self.foo(nltk.pos_tag(pred_list))\n",
    "            target_tag = self.foo(nltk.pos_tag(target_list))\n",
    "            str1 = ' '.join(pred_tag)  # Convert lists to strings\n",
    "            str2 = ' '.join(target_tag)\n",
    "            distance = Levenshtein.distance(str1, str2)\n",
    "            dist = torch.Tensor([distance])\n",
    "        except:\n",
    "            dist = torch.Tensor([2*len(target_list)])\n",
    "        return dist\n",
    "    def loss_func2(self, word, y):\n",
    "        if word =='NA':\n",
    "            return  torch.full((1,), fill_value=100)\n",
    "        nlp = en_core_web_sm.load()\n",
    "        pred_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', word)\n",
    "        target_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', y)\n",
    "        try:\n",
    "            str1 = ' '.join(pred_list)  # Convert lists to strings\n",
    "            str2 = ' '.join(target_list)\n",
    "            tokens1 = nlp(str1)\n",
    "            tokens2 = nlp(str2)\n",
    "            # Calculate the average word embedding for each string\n",
    "            embedding1 = sum(token.vector for token in tokens1) / len(tokens1)\n",
    "            embedding2 = sum(token.vector for token in tokens2) / len(tokens2)\n",
    "            # Calculate the cosine similarity between the embeddings\n",
    "            w1= LA.norm(embedding1)\n",
    "            w2= LA.norm(embedding2)\n",
    "            distance = 1 - (embedding1.dot(embedding2) / (w1 * w2))\n",
    "            dist = torch.Tensor([distance])\n",
    "        except:\n",
    "            dist = torch.Tensor([1])\n",
    "        return dist\n",
    "    def compute_loss(self, logits, target_word):\n",
    "        # Apply softmax to obtain probabilities\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        log_probs = torch.log(probabilities)\n",
    "        target_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', target_word)\n",
    "        joined_sentence = \" \".join(target_list)\n",
    "        target_tokens = self.tokenizer.tokenize(joined_sentence)\n",
    "        loss = 0\n",
    "        for j in range(len(target_tokens)):\n",
    "            target_index = self.tokenizer.convert_tokens_to_ids(target_tokens[j])\n",
    "            # Retrieve the probability of the target word\n",
    "            target_prob = probabilities[:, target_index]  # Assuming target_index is known\n",
    "            # Compute the negative log-likelihood loss\n",
    "            l = -torch.log(target_prob)\n",
    "            loss+=l\n",
    "        return {'loss':loss,'log_probs':log_probs}\n",
    "    def forward(self, mapi):\n",
    "        english_dict = set(wal.words())\n",
    "        X_init = mapi[0]\n",
    "        y = mapi[1]\n",
    "        print(y)\n",
    "        nl = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', y)\n",
    "        lb = ' '.join(nl).lower()\n",
    "        x = tokenizer.tokenize(lb)\n",
    "        num_sub_tokens_label = len(x)\n",
    "        X_init = X_init.replace(\"[MASK]\", \" \".join([tokenizer.mask_token] * num_sub_tokens_label))\n",
    "        preds = []\n",
    "        for m in range(num_sub_tokens_label):\n",
    "            print(m)\n",
    "            tokens = self.tokenizer.encode_plus(X_init, add_special_tokens=False,return_tensors='pt')\n",
    "            input_id_chunki = tokens['input_ids'][0].split(510)\n",
    "            input_id_chunks = []\n",
    "            mask_chunks  = []\n",
    "            mask_chunki = tokens['attention_mask'][0].split(510)\n",
    "            for tensor in input_id_chunki:\n",
    "                input_id_chunks.append(tensor)\n",
    "            for tensor in mask_chunki:\n",
    "                mask_chunks.append(tensor)\n",
    "            xi = torch.full((1,), fill_value=101)\n",
    "            yi = torch.full((1,), fill_value=1)\n",
    "            zi = torch.full((1,), fill_value=102)\n",
    "            for r in range(len(input_id_chunks)):\n",
    "                input_id_chunks[r] = torch.cat([xi, input_id_chunks[r]],dim = -1)\n",
    "                input_id_chunks[r] = torch.cat([input_id_chunks[r],zi],dim=-1)\n",
    "                mask_chunks[r] = torch.cat([yi, mask_chunks[r]],dim=-1)\n",
    "                mask_chunks[r] = torch.cat([mask_chunks[r],yi],dim=-1)\n",
    "            di = torch.full((1,), fill_value=0)\n",
    "            for i in range(len(input_id_chunks)):\n",
    "                # get required padding length\n",
    "                pad_len = 512 - input_id_chunks[i].shape[0]\n",
    "                # check if tensor length satisfies required chunk size\n",
    "                if pad_len > 0:\n",
    "                    # if padding length is more than 0, we must add padding\n",
    "                    for p in range(pad_len):\n",
    "                        input_id_chunks[i] = torch.cat([input_id_chunks[i],di],dim=-1)\n",
    "                        mask_chunks[i] = torch.cat([mask_chunks[i],di],dim=-1)\n",
    "            input_ids = torch.stack(input_id_chunks)\n",
    "            attention_mask = torch.stack(mask_chunks)\n",
    "            input_dict = {\n",
    "                'input_ids': input_ids.long(),\n",
    "                'attention_mask': attention_mask.int()\n",
    "            }\n",
    "            maski = []\n",
    "            u = 0\n",
    "            ad = 0\n",
    "            for l in range(len(input_dict['input_ids'])):\n",
    "                masked_pos = []\n",
    "                for i in range(len(input_dict['input_ids'][l])):\n",
    "                    if input_dict['input_ids'][l][i] == 50264: #103\n",
    "                        u+=1\n",
    "                        if i != 0 and input_dict['input_ids'][l][i-1] == 50264:\n",
    "                            continue\n",
    "                        masked_pos.append(i)\n",
    "                        ad+=1\n",
    "                maski.append(masked_pos)\n",
    "#             if u<8:\n",
    "#                 print(\"maski: \", maski)\n",
    "            print('number of mask tok',u)\n",
    "            print('number of seq', ad)\n",
    "            with torch.no_grad():\n",
    "                output = self.model(**input_dict)\n",
    "            last_hidden_state = output[0].squeeze()\n",
    "            l_o_l_sa = []\n",
    "            lhs_agg = []\n",
    "            if len(maski) == 1:\n",
    "                masked_pos = maski[0]\n",
    "                lhs_agg.append(last_hidden_state)\n",
    "                for k in masked_pos:\n",
    "                    l_o_l_sa.append(last_hidden_state[k])\n",
    "            else:\n",
    "                for p in range(len(maski)):\n",
    "                    lhs_agg.append(last_hidden_state[p])\n",
    "                    masked_pos = maski[p]\n",
    "                    for k in masked_pos:\n",
    "                        l_o_l_sa.append(last_hidden_state[p][k])\n",
    "            sum_state = l_o_l_sa[0]\n",
    "            lhs = lhs_agg[0]\n",
    "            for i in range(len(lhs_agg)):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                lhs+=lhs_agg[i]\n",
    "            lhs/=len(lhs_agg)\n",
    "            for i in range(len(l_o_l_sa)):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                sum_state += l_o_l_sa[i]\n",
    "            yip = len(l_o_l_sa)\n",
    "            sum_state /= yip\n",
    "    #         try:\n",
    "            idx = torch.topk(sum_state, k=1, dim=0)[1]\n",
    "            qw = [self.tokenizer.decode(i.item()).strip() for i in idx][0]\n",
    "            preds.append(qw)\n",
    "            xl = X_init.split()\n",
    "            xxl = []\n",
    "            for p in range(len(xl)):\n",
    "                if xl[p] == tokenizer.mask_token:\n",
    "                    if p != 0 and xl[p-1] == tokenizer.mask_token:\n",
    "                        xxl.append(xl[p])\n",
    "                        continue\n",
    "                    xxl.append(qw)\n",
    "                    continue\n",
    "                xxl.append(xl[p])\n",
    "            X_init = \" \".join(xxl)\n",
    "        we = preds[0]\n",
    "        for t in range(len(preds)):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            we+=preds[t].capitalize()\n",
    "\n",
    "        word_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', we)\n",
    "        z = 0\n",
    "        while z < len(word_list) - 1:\n",
    "            word1 = word_list[z].lower()\n",
    "            word2 = word_list[z + 1].lower()\n",
    "            merged_word = word1 + word2\n",
    "\n",
    "            if word1 not in english_dict and word2 not in english_dict:\n",
    "                # Merge the 2 words and insert the resulting word at index (z)\n",
    "                word_list[z] = merged_word\n",
    "                word_list.pop(z + 1)\n",
    "\n",
    "            elif word1 in english_dict and word2 not in english_dict:\n",
    "                # Combine the words to see if the resulting word is in the dictionary\n",
    "                if merged_word in english_dict:\n",
    "                    # Merge the words and insert the merged word at index (z)\n",
    "                    word_list[z] = merged_word\n",
    "                    word_list.pop(z + 1)\n",
    "                else:\n",
    "                    if not (z+2)<len(word_list):\n",
    "                        z+=1\n",
    "                        continue\n",
    "                    a = merged_word+word_list[z + 2].lower()\n",
    "                    if a in english_dict:\n",
    "                        word_list[z] = a\n",
    "                        word_list.pop(z + 1)\n",
    "                        word_list.pop(z + 2)\n",
    "                    else:\n",
    "                        z+=1\n",
    "                        continue\n",
    "            elif word1 not in english_dict and word2 in english_dict:\n",
    "                # Combine the words to see if the resulting word is in the dictionary\n",
    "                if merged_word in english_dict:\n",
    "                    # Merge the words and insert the merged word at index (z)\n",
    "                    word_list[z] = merged_word\n",
    "                    word_list.pop(z + 1)\n",
    "                else:\n",
    "                    if not (z+2)<len(word_list):\n",
    "                        z+=1\n",
    "                        continue\n",
    "                    a = merged_word+word_list[z + 2].lower()\n",
    "                    if a in english_dict:\n",
    "                        word_list[z] = a\n",
    "                        word_list.pop(z + 1)\n",
    "                        word_list.pop(z + 2)\n",
    "                    else:\n",
    "                        z+=1\n",
    "                        continue\n",
    "            else:\n",
    "                z += 1\n",
    "                continue\n",
    "            z+=1\n",
    "        fin_str = \"\"\n",
    "        for o in range(len(word_list)):\n",
    "            if o == 0:\n",
    "                fin_str+=word_list[o].lower()\n",
    "                continue\n",
    "            fin_str+=word_list[o].lower().capitalize()\n",
    "        word = fin_str\n",
    "#         except:\n",
    "#             word = \"NA\"         \n",
    "        pred_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', word)\n",
    "        target_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', y)\n",
    "        target_string = ' '.join(pred_list)\n",
    "        predicted_string = ' '.join(target_list)\n",
    "        precision, recall, f1_score, _ = precision_recall_fscore_support([target_string], [predicted_string], average='micro')\n",
    "        rank = 0\n",
    "        for d, wordh in enumerate(pred_list):\n",
    "            if d >= len(target_list):\n",
    "                break\n",
    "            if wordh == target_list[d]:\n",
    "                rank = d + 1\n",
    "                break\n",
    "        mrr = 1.0 / rank if rank > 0 else 0.0\n",
    "        target_set = set(target_list)\n",
    "        predicted_set = set(pred_list)\n",
    "\n",
    "        # Calculate Precision at K for K=3 (top 3 predictions)\n",
    "        K = 3\n",
    "        top_k_predictions = pred_list[:K]\n",
    "\n",
    "        # Count the number of correct predictions in the top K\n",
    "        correct_predictions = sum(1 for word in top_k_predictions if word in target_set)\n",
    "\n",
    "        # Calculate Precision at K\n",
    "        pak = correct_predictions / K\n",
    "        print (\"Guess : \",word)\n",
    "#         maxi = Variable(torch.tensor(0.5, dtype=torch.float), requires_grad = True)\n",
    "        maxi = Variable(0.2*self.loss_func2(word,y) + 0.8*self.loss_func1(word,y), requires_grad = True)\n",
    "#         maxi.requires_grad()\n",
    "        \n",
    "        \n",
    "        logits = self.linear_layer(lhs)\n",
    "        cl = self.compute_loss(logits,y)\n",
    "        if len(old_inp) == 0:\n",
    "            old_log_probs = torch.rand_like(cl['log_probs'])\n",
    "            logits1 = torch.rand_like(logits)\n",
    "            hits = torch.rand_like(last_hidden_state[0])\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                op = self.model(**self.oi[-1])\n",
    "            self.oi.clear()\n",
    "            self.oi.append(input_dict)\n",
    "            last_hidden_state1 = op[0].squeeze()\n",
    "            lhs_agg1 = []\n",
    "            if len(maski) == 1:\n",
    "                lhs_agg1.append(last_hidden_state1)\n",
    "            else:\n",
    "                for p in range(len(maski)):\n",
    "                    lhs_agg1.append(last_hidden_state1[p])\n",
    "            lhs1 = lhs_agg1[0]\n",
    "            for i in range(len(lhs_agg1)):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                lhs1+=lhs_agg1[i]\n",
    "            lhs1/=len(lhs_agg1)\n",
    "            logits1 = self.linear_layer(lhs1)\n",
    "            old_probabilities = F.softmax(logits1, dim=-1)\n",
    "            old_log_probs = torch.log(old_probabilities)\n",
    "            hits = self.old_mhs[-1]\n",
    "            if len(self.old_mhs == 5):\n",
    "                self.old_mhs.clear()\n",
    "            self.old_mhs.append(sum_state)\n",
    "            \n",
    "        return {'pak':pak,'mrr':mrr,'f1':f1_score,'returned_word':word, 'mhs':sum_state,'old_mhs':hits,'old_logits':logits1, 'actual_pred':word, 'loss':maxi, 'compute_loss':cl['loss'],'log_probs':cl['log_probs'],'logits':logits, 'old_log_probs':old_log_probs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "368ea55e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T06:53:33.275676Z",
     "start_time": "2023-08-15T06:53:33.271512Z"
    }
   },
   "outputs": [],
   "source": [
    "flag = 0\n",
    "def train_one_epoch(transformer_model, epoch_index, tb_writer, dataset,scheduler):\n",
    "    global flag\n",
    "    global myDs\n",
    "    f1 = 0\n",
    "    mrr = 0\n",
    "    pak = 0\n",
    "    for batch in dataset:\n",
    "        p = 0\n",
    "        inputs = batch\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            for i in range(len(inputs[0])):\n",
    "                l = []\n",
    "                l.append(inputs[0][i])\n",
    "                l.append(inputs[1][i])\n",
    "                opi = transformer_model(l)\n",
    "                pred = opi['actual_pred']\n",
    "                if pred == 'NA':\n",
    "                    print(\"a\")\n",
    "                    continue\n",
    "                loss1 = opi['loss']\n",
    "                f1 += opi['f1']\n",
    "                mrr+=opi['mrr']\n",
    "                pak+=opi['pak']\n",
    "                if p == 0:\n",
    "                    loss = Variable(loss1, requires_grad = True)\n",
    "                    p+=1\n",
    "                else:\n",
    "                    loss = torch.cat([loss, loss1],dim = -1)\n",
    "                    p+=1\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if p % 1 == 0:\n",
    "            print('  batch loss: {}'.format(loss))\n",
    "    print(\" F1: \"+str(f1))\n",
    "    print(\" MRR: \"+str(mrr))\n",
    "    print(\" P@3: \"+str(pak))\n",
    "    l = len(myDs)\n",
    "    f1 /= l\n",
    "    mrr /= l\n",
    "    pak /= l\n",
    "    print(\"Avg F1: \"+str(f1))\n",
    "    print(\"Avg MRR: \"+str(mrr))\n",
    "    print(\"Avg P@3: \"+str(pak))\n",
    "    return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a075c13c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T06:53:43.867837Z",
     "start_time": "2023-08-15T06:53:33.276785Z"
    }
   },
   "outputs": [],
   "source": [
    "epoch_number = 0\n",
    "EPOCHS = 5\n",
    "run_int = 98\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/trainer_{}'.format(timestamp))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "model = Step1_model()\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "num_training_steps = 5332  # Adjust this based on the number of training steps\n",
    "num_warmup_steps = 733  # 10-20% of num_training_steps\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "myDs=MyDataset('dat.csv')\n",
    "# vDs=MyDataset('valid_sing_cand.csv')\n",
    "train_loader=DataLoader(myDs,batch_size=8,shuffle=True)\n",
    "# validation_loader=DataLoader(vDs,batch_size=1,shuffle=False)\n",
    "best_loss = torch.full((1,), fill_value=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7609a233",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-15T06:54:47.485Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2557 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "checkConfigurationMetadata\n",
      "\n",
      "0\n",
      "number of mask tok 18\n",
      "number of seq 6\n",
      "1\n",
      "number of mask tok 12\n",
      "number of seq 6\n",
      "2\n",
      "number of mask tok 6\n",
      "number of seq 6\n",
      "WL1:  ['Check', 'Provider']\n",
      "checkProvider\n",
      "Guess :  checkProvider\n",
      "xAxisFormatter\n",
      "\n",
      "0\n",
      "number of mask tok 12\n",
      "number of seq 3\n",
      "1\n",
      "number of mask tok 9\n",
      "number of seq 3\n",
      "2\n",
      "number of mask tok 6\n",
      "number of seq 3\n",
      "3\n",
      "number of mask tok 3\n",
      "number of seq 3\n",
      "WL1:  ['1']\n",
      "1\n",
      "Guess :  1\n",
      "nothing\n",
      "\n",
      "0\n",
      "number of mask tok 2\n",
      "number of seq 2\n",
      "WL1:  ['empty']\n",
      "empty\n",
      "Guess :  empty\n",
      "valuesCopy\n",
      "\n",
      "0\n",
      "number of mask tok 4\n",
      "number of seq 2\n",
      "1\n",
      "number of mask tok 2\n",
      "number of seq 2\n",
      "WL1:  ['values', 'List']\n",
      "valuesList\n",
      "Guess :  valuesList\n",
      "observer\n",
      "\n",
      "0\n",
      "number of mask tok 4\n",
      "number of seq 2\n",
      "1\n",
      "number of mask tok 2\n",
      "number of seq 2\n",
      "WL1:  ['A']\n",
      "a\n",
      "Guess :  a\n",
      "jvmArgs\n",
      "\n",
      "0\n",
      "number of mask tok 12\n",
      "number of seq 4\n",
      "1\n",
      "number of mask tok 8\n",
      "number of seq 4\n",
      "2\n",
      "number of mask tok 4\n",
      "number of seq 4\n",
      "WL1:  ['Set', 'Args']\n",
      "setArgs\n",
      "Guess :  setArgs\n",
      "testers\n",
      "\n",
      "0\n",
      "number of mask tok 7\n",
      "number of seq 7\n",
      "WL1:  []\n",
      "\n",
      "Guess :  \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(model, epoch_number, writer, train_loader,scheduler)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "#     running_vloss = 0.0\n",
    "#     for i, vdata in enumerate(validation_loader):\n",
    "#         try:\n",
    "#             vinputs, vlabels = vdata\n",
    "#             voutputs = model(vinputs)\n",
    "#             vloss = loss_fn(voutputs, vlabels)\n",
    "#             running_vloss += vloss\n",
    "#         except:\n",
    "#             flag+=1\n",
    "#             continue\n",
    "\n",
    "#     avg_vloss = running_vloss / (i + 1)\n",
    "#     print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    print('LOSS train {}'.format(avg_loss))\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "#     writer.add_scalars('Training vs. Validation Loss',\n",
    "#                     { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "#                     epoch_number + 1)\n",
    "#     writer.add_scalars('Training Loss',\n",
    "#                     { 'Training' : avg_loss},\n",
    "#                     epoch_number + 1)\n",
    "#     writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        model_path = 'var_runs/model_{}_{}'.format(run_int, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "#     if avg_vloss < best_vloss:\n",
    "#         best_vloss = avg_vloss\n",
    "#         model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "#         torch.save(model.state_dict(), model_path)\n",
    "\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4fd33c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflown)",
   "language": "python",
   "name": "tensorflown"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
