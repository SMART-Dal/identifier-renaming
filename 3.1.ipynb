{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "219c729b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T17:39:31.676954Z",
     "start_time": "2023-09-02T17:39:31.660826Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init, MarginRankingLoss\n",
    "from transformers import BertModel, RobertaModel\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "from torch.optim import Adam\n",
    "from distutils.version import LooseVersion\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from torch.autograd import Variable\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "import nltk\n",
    "import re\n",
    "import Levenshtein\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from numpy import linalg as LA\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from nltk.corpus import wordnet\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init, MarginRankingLoss\n",
    "from transformers import BertModel, RobertaModel\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "from torch.optim import Adam\n",
    "from distutils.version import LooseVersion\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from torch.autograd import Variable\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "import nltk\n",
    "import re\n",
    "import Levenshtein\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from numpy import linalg as LA\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from nltk.corpus import wordnet\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from nltk.corpus import words as wal\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"words\")\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13ae6ea8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T05:35:08.391407Z",
     "start_time": "2023-08-23T05:35:08.389532Z"
    }
   },
   "outputs": [],
   "source": [
    "def freeze(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = True\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.0\"):\n",
    "#             param.requires_grad = False\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.1\"):\n",
    "#             param.requires_grad = False\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.2\"):\n",
    "#             param.requires_grad = False\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.3\"):\n",
    "#             param.requires_grad = False\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.4\"):\n",
    "#             param.requires_grad = False\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.5\"):\n",
    "#             param.requires_grad = False\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.6\"):\n",
    "#             param.requires_grad = False\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.7\"):\n",
    "#             param.requires_grad = False\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.8\"):\n",
    "#             param.requires_grad = False\n",
    "#         if name.startswith(\"model.roberta.encoder.layer.9\"):\n",
    "#             param.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c1e0f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T05:35:08.394315Z",
     "start_time": "2023-08-23T05:35:08.392280Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,file_name):\n",
    "        df1 = pd.read_csv(file_name)\n",
    "        df1 = df1.fillna(\"\")\n",
    "        res = df1['X']\n",
    "#         ab = df1['X']\n",
    "#         res = [sub.replace(\"<mask>\", \"[MASK]\") for sub in ab]\n",
    "        self.X_list = res\n",
    "        self.y_list = df1['y']\n",
    "        self.clean = df1['clean_text']\n",
    "    def __len__(self):\n",
    "        return len(self.X_list)\n",
    "    def __getitem__(self,idx):\n",
    "        mapi = []\n",
    "        mapi.append(self.X_list[idx])\n",
    "        mapi.append(self.y_list[idx])\n",
    "        mapi.append(self.clean[idx])\n",
    "        return mapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bc7484f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T05:35:08.416583Z",
     "start_time": "2023-08-23T05:35:08.395711Z"
    }
   },
   "outputs": [],
   "source": [
    "class Step1_model(nn.Module):\n",
    "    def __init__(self, hidden_size=512):\n",
    "        self.old_mhs = []\n",
    "        self.old_log_probs = []\n",
    "        self.old_logits = []\n",
    "        super(Step1_model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "#         self.model = AutoModel.from_pretrained(\"roberta-base\")\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "#         self.config = AutoConfig.from_pretrained(\"roberta-base\")\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained('microsoft/graphcodebert-base')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "        self.config = AutoConfig.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "        self.linear_layer = nn.Linear(self.model.config.vocab_size, self.model.config.vocab_size)\n",
    "\n",
    "#         self.model = AutoModelForMaskedLM.from_pretrained('bert-base-cased')\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "#         self.config = AutoConfig.from_pretrained(\"bert-base-cased\")\n",
    "        for param in self.model.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "    def foo (self,data):\n",
    "        result = []\n",
    "        if type(data) == tuple:\n",
    "            return data[1]\n",
    "        if type(data) == list:\n",
    "            for inner in data:\n",
    "                result.append(foo(inner))\n",
    "        res = []\n",
    "        for a in result[0]:\n",
    "            res.append(a[:2])\n",
    "        return res\n",
    "    def loss_func1(self, word, y):\n",
    "        if word =='NA':\n",
    "            return torch.full((1,), fill_value=100)\n",
    "        try:\n",
    "            pred_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', word)\n",
    "            target_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', y)\n",
    "            pred_tag = self.foo(nltk.pos_tag(pred_list))\n",
    "            target_tag = self.foo(nltk.pos_tag(target_list))\n",
    "            str1 = ' '.join(pred_tag)  # Convert lists to strings\n",
    "            str2 = ' '.join(target_tag)\n",
    "            distance = Levenshtein.distance(str1, str2)\n",
    "            dist = torch.Tensor([distance])\n",
    "        except:\n",
    "            dist = torch.Tensor([2*len(target_list)])\n",
    "        return dist\n",
    "    def loss_func2(self, word, y):\n",
    "        if word =='NA':\n",
    "            return  torch.full((1,), fill_value=100)\n",
    "        nlp = en_core_web_sm.load()\n",
    "        pred_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', word)\n",
    "        target_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', y)\n",
    "        try:\n",
    "            str1 = ' '.join(pred_list)  # Convert lists to strings\n",
    "            str2 = ' '.join(target_list)\n",
    "            tokens1 = nlp(str1)\n",
    "            tokens2 = nlp(str2)\n",
    "            # Calculate the average word embedding for each string\n",
    "            embedding1 = sum(token.vector for token in tokens1) / len(tokens1)\n",
    "            embedding2 = sum(token.vector for token in tokens2) / len(tokens2)\n",
    "            # Calculate the cosine similarity between the embeddings\n",
    "            w1= LA.norm(embedding1)\n",
    "            w2= LA.norm(embedding2)\n",
    "            distance = 1 - (embedding1.dot(embedding2) / (w1 * w2))\n",
    "            dist = torch.Tensor([distance])\n",
    "        except:\n",
    "            dist = torch.Tensor([1])\n",
    "        return dist\n",
    "    def compute_loss(self, logits, target_word):\n",
    "        # Apply softmax to obtain probabilities\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        log_probs = torch.log(probabilities)\n",
    "        target_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', target_word)\n",
    "        joined_sentence = \" \".join(target_list)\n",
    "        target_tokens = self.tokenizer.tokenize(joined_sentence)\n",
    "        loss = 0\n",
    "        for j in range(len(target_tokens)):\n",
    "            target_index = self.tokenizer.convert_tokens_to_ids(target_tokens[j])\n",
    "            # Retrieve the probability of the target word\n",
    "            target_prob = probabilities[:, target_index]  # Assuming target_index is known\n",
    "            # Compute the negative log-likelihood loss\n",
    "            l = -torch.log(target_prob)\n",
    "            loss+=l\n",
    "        return {'loss':loss,'log_probs':log_probs}\n",
    "    def forward(self, mapi):\n",
    "        english_dict = set(wal.words())\n",
    "        X_init = mapi[0]\n",
    "        y = mapi[1]\n",
    "        print(y)\n",
    "        nl = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', y)\n",
    "        lb = ' '.join(nl).lower()\n",
    "        x = self.tokenizer.tokenize(lb)\n",
    "        num_sub_tokens_label = len(x)\n",
    "        X_init = X_init.replace(\"[MASK]\", \" \".join([self.tokenizer.mask_token] * num_sub_tokens_label))\n",
    "        preds = []\n",
    "        for m in range(num_sub_tokens_label):\n",
    "            print(m)\n",
    "            tokens = self.tokenizer.encode_plus(X_init, add_special_tokens=False,return_tensors='pt')\n",
    "            input_id_chunki = tokens['input_ids'][0].split(510)\n",
    "            input_id_chunks = []\n",
    "            mask_chunks  = []\n",
    "            mask_chunki = tokens['attention_mask'][0].split(510)\n",
    "            for tensor in input_id_chunki:\n",
    "                input_id_chunks.append(tensor)\n",
    "            for tensor in mask_chunki:\n",
    "                mask_chunks.append(tensor)\n",
    "            xi = torch.full((1,), fill_value=101)\n",
    "            yi = torch.full((1,), fill_value=1)\n",
    "            zi = torch.full((1,), fill_value=102)\n",
    "            for r in range(len(input_id_chunks)):\n",
    "                input_id_chunks[r] = torch.cat([xi, input_id_chunks[r]],dim = -1)\n",
    "                input_id_chunks[r] = torch.cat([input_id_chunks[r],zi],dim=-1)\n",
    "                mask_chunks[r] = torch.cat([yi, mask_chunks[r]],dim=-1)\n",
    "                mask_chunks[r] = torch.cat([mask_chunks[r],yi],dim=-1)\n",
    "            di = torch.full((1,), fill_value=0)\n",
    "            for i in range(len(input_id_chunks)):\n",
    "                # get required padding length\n",
    "                pad_len = 512 - input_id_chunks[i].shape[0]\n",
    "                # check if tensor length satisfies required chunk size\n",
    "                if pad_len > 0:\n",
    "                    # if padding length is more than 0, we must add padding\n",
    "                    for p in range(pad_len):\n",
    "                        input_id_chunks[i] = torch.cat([input_id_chunks[i],di],dim=-1)\n",
    "                        mask_chunks[i] = torch.cat([mask_chunks[i],di],dim=-1)\n",
    "            input_ids = torch.stack(input_id_chunks)\n",
    "            attention_mask = torch.stack(mask_chunks)\n",
    "            input_dict = {\n",
    "                'input_ids': input_ids.long(),\n",
    "                'attention_mask': attention_mask.int()\n",
    "            }\n",
    "            maski = []\n",
    "            u = 0\n",
    "            ad = 0\n",
    "            for l in range(len(input_dict['input_ids'])):\n",
    "                masked_pos = []\n",
    "                for i in range(len(input_dict['input_ids'][l])):\n",
    "                    if input_dict['input_ids'][l][i] == 50264: #103\n",
    "                        u+=1\n",
    "                        if i != 0 and input_dict['input_ids'][l][i-1] == 50264:\n",
    "                            continue\n",
    "                        masked_pos.append(i)\n",
    "                        ad+=1\n",
    "                maski.append(masked_pos)\n",
    "#             if u<8:\n",
    "#                 print(\"maski: \", maski)\n",
    "            print('number of mask tok',u)\n",
    "            print('number of seq', ad)\n",
    "            with torch.no_grad():\n",
    "                output = self.model(**input_dict)\n",
    "            last_hidden_state = output[0].squeeze()\n",
    "            l_o_l_sa = []\n",
    "            lhs_agg = []\n",
    "            if len(maski) == 1:\n",
    "                masked_pos = maski[0]\n",
    "                lhs_agg.append(last_hidden_state)\n",
    "                for k in masked_pos:\n",
    "                    l_o_l_sa.append(last_hidden_state[k])\n",
    "            else:\n",
    "                for p in range(len(maski)):\n",
    "                    lhs_agg.append(last_hidden_state[p])\n",
    "                    masked_pos = maski[p]\n",
    "                    for k in masked_pos:\n",
    "                        l_o_l_sa.append(last_hidden_state[p][k])\n",
    "            sum_state = l_o_l_sa[0]\n",
    "            lhs = lhs_agg[0]\n",
    "            for i in range(len(lhs_agg)):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                lhs+=lhs_agg[i]\n",
    "            lhs/=len(lhs_agg)\n",
    "            for i in range(len(l_o_l_sa)):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                sum_state += l_o_l_sa[i]\n",
    "            yip = len(l_o_l_sa)\n",
    "            sum_state /= yip\n",
    "    #         try:\n",
    "            idx = torch.topk(sum_state, k=1, dim=0)[1]\n",
    "            qw = [self.tokenizer.decode(i.item()).strip() for i in idx][0]\n",
    "            preds.append(qw)\n",
    "            xl = X_init.split()\n",
    "            xxl = []\n",
    "            for p in range(len(xl)):\n",
    "                if xl[p] == self.tokenizer.mask_token:\n",
    "                    if p != 0 and xl[p-1] == self.tokenizer.mask_token:\n",
    "                        xxl.append(xl[p])\n",
    "                        continue\n",
    "                    xxl.append(qw)\n",
    "                    continue\n",
    "                xxl.append(xl[p])\n",
    "            X_init = \" \".join(xxl)\n",
    "        we = preds[0]\n",
    "        for t in range(len(preds)):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            we+=preds[t].capitalize()\n",
    "\n",
    "        word_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', we)\n",
    "        z = 0\n",
    "        while z < len(word_list) - 1:\n",
    "            word1 = word_list[z].lower()\n",
    "            word2 = word_list[z + 1].lower()\n",
    "            merged_word = word1 + word2\n",
    "\n",
    "            if word1 not in english_dict and word2 not in english_dict:\n",
    "                # Merge the 2 words and insert the resulting word at index (z)\n",
    "                word_list[z] = merged_word\n",
    "                word_list.pop(z + 1)\n",
    "\n",
    "            elif word1 in english_dict and word2 not in english_dict:\n",
    "                # Combine the words to see if the resulting word is in the dictionary\n",
    "                if merged_word in english_dict:\n",
    "                    # Merge the words and insert the merged word at index (z)\n",
    "                    word_list[z] = merged_word\n",
    "                    word_list.pop(z + 1)\n",
    "                else:\n",
    "                    if not (z+2)<len(word_list):\n",
    "                        z+=1\n",
    "                        continue\n",
    "                    a = merged_word+word_list[z + 2].lower()\n",
    "                    if a in english_dict:\n",
    "                        word_list[z] = a\n",
    "                        word_list.pop(z + 1)\n",
    "                        word_list.pop(z + 2)\n",
    "                    else:\n",
    "                        z+=1\n",
    "                        continue\n",
    "            elif word1 not in english_dict and word2 in english_dict:\n",
    "                # Combine the words to see if the resulting word is in the dictionary\n",
    "                if merged_word in english_dict:\n",
    "                    # Merge the words and insert the merged word at index (z)\n",
    "                    word_list[z] = merged_word\n",
    "                    word_list.pop(z + 1)\n",
    "                else:\n",
    "                    if not (z+2)<len(word_list):\n",
    "                        z+=1\n",
    "                        continue\n",
    "                    a = merged_word+word_list[z + 2].lower()\n",
    "                    if a in english_dict:\n",
    "                        word_list[z] = a\n",
    "                        word_list.pop(z + 1)\n",
    "                        word_list.pop(z + 2)\n",
    "                    else:\n",
    "                        z+=1\n",
    "                        continue\n",
    "            else:\n",
    "                z += 1\n",
    "                continue\n",
    "            z+=1\n",
    "        fin_str = \"\"\n",
    "        for o in range(len(word_list)):\n",
    "            if o == 0:\n",
    "                fin_str+=word_list[o].lower()\n",
    "                continue\n",
    "            fin_str+=word_list[o].lower().capitalize()\n",
    "        word = fin_str\n",
    "#         except:\n",
    "#             word = \"NA\"         \n",
    "        pred_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', word)\n",
    "        target_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', y)\n",
    "        target_string = ' '.join(pred_list)\n",
    "        predicted_string = ' '.join(target_list)\n",
    "        precision, recall, f1_score, _ = precision_recall_fscore_support([target_string], [predicted_string], average='micro')\n",
    "        rank = 0\n",
    "        for d, wordh in enumerate(pred_list):\n",
    "            if d >= len(target_list):\n",
    "                break\n",
    "            if wordh == target_list[d]:\n",
    "                rank = d + 1\n",
    "                break\n",
    "        mrr = 1.0 / rank if rank > 0 else 0.0\n",
    "        target_set = set(target_list)\n",
    "        predicted_set = set(pred_list)\n",
    "\n",
    "        # Calculate Precision at K for K=3 (top 3 predictions)\n",
    "        K = 3\n",
    "        top_k_predictions = pred_list[:K]\n",
    "\n",
    "        # Count the number of correct predictions in the top K\n",
    "        correct_predictions = sum(1 for word in top_k_predictions if word in target_set)\n",
    "\n",
    "        # Calculate Precision at K\n",
    "        pak = correct_predictions / K\n",
    "        print (\"Guess : \",word)\n",
    "#         maxi = Variable(torch.tensor(0.5, dtype=torch.float), requires_grad = True)\n",
    "        maxi = Variable(0.2*self.loss_func2(word,y) + 0.8*self.loss_func1(word,y), requires_grad = True)\n",
    "#         maxi.requires_grad()\n",
    "        \n",
    "        \n",
    "        logits = self.linear_layer(lhs)\n",
    "        cl = self.compute_loss(logits,y)\n",
    "        if len(self.old_mhs) == 0:\n",
    "            print(\"one\")\n",
    "            old_log_probs = torch.rand_like(cl['log_probs'])\n",
    "            logits1 = torch.rand_like(logits)\n",
    "            self.old_mhs.append(sum_state)\n",
    "            self.old_log_probs.append(cl['log_probs'])\n",
    "            self.old_logits.append(logits)\n",
    "            if last_hidden_state[0].shape[0] < 1000:\n",
    "                hits = torch.rand_like(last_hidden_state[0][0])\n",
    "            else:\n",
    "                hits = torch.rand_like(last_hidden_state[0])\n",
    "        else:\n",
    "            old_log_probs = self.old_log_probs[-1]\n",
    "            logits1 = self.old_logits[-1]\n",
    "            hits = self.old_mhs[-1]\n",
    "            if len(self.old_mhs) == 1:\n",
    "                self.old_mhs.clear()\n",
    "            if len(self.old_logits) == 1:\n",
    "                self.old_logits.clear()\n",
    "            if len(self.old_log_probs) == 1:\n",
    "                self.old_log_probs.clear()\n",
    "            self.old_mhs.append(sum_state)\n",
    "            self.old_log_probs.append(cl['log_probs'])\n",
    "            self.old_logits.append(logits)\n",
    "            \n",
    "        return {'pak':pak,'mrr':mrr,'f1':f1_score,'returned_word':word, 'mhs':sum_state,'old_mhs':hits,'old_logits':logits1, 'actual_pred':word, 'loss':maxi, 'compute_loss':cl['loss'],'log_probs':cl['log_probs'],'logits':logits, 'old_log_probs':old_log_probs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca7a001a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T05:35:08.421218Z",
     "start_time": "2023-08-23T05:35:08.417863Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_comments(java_code):\n",
    "    pattern = r\"(?<!\\S)(\\/\\/[^\\r\\n]*|\\/\\*(?:(?!\\*\\/).|[\\r\\n])*?\\*\\/)\"\n",
    "    comments = re.findall(pattern, java_code)\n",
    "#     print(java_code)\n",
    "    mask_indices = [m.start() for m in re.finditer(r\"[MASK]\", java_code)]\n",
    "    pt = [m.start() for m in re.finditer(pattern, java_code)]\n",
    "    if len(mask_indices) == 0:\n",
    "        return []\n",
    "    fir = mask_indices[0]\n",
    "    las = mask_indices[-1]\n",
    "    keep = []\n",
    "    for i in range(len(pt)):\n",
    "        if pt[i]>=fir and pt[i]<=las:\n",
    "            keep.append(i)\n",
    "    try:\n",
    "        if (keep[0] - 1) >=0:\n",
    "            keep.append(keep[0] - 1)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        if (len(pt) - 1) >= keep[-1]:\n",
    "            keep.append(keep[-1]+1)\n",
    "    except:\n",
    "        pass\n",
    "    ret = []\n",
    "    if 0 not in keep:\n",
    "        if len(comments) != 0:\n",
    "            ret.append(comments[0])\n",
    "    for k in keep:\n",
    "        if len(comments) > k:\n",
    "            ret.append(comments[k])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7e3a3c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T05:35:08.425270Z",
     "start_time": "2023-08-23T05:35:08.422153Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_co_occurrence_probability(str1, word2):\n",
    "    # use graphCodeBert or Bert\n",
    "    model = AutoModelForMaskedLM.from_pretrained('microsoft/graphcodebert-base')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "    config = AutoConfig.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "\n",
    "    # Tokenize the sentence with the two words to create input tensors\n",
    "    x = tokenizer.tokenize(word2)\n",
    "    num_sub_tokens_label = len(x)\n",
    "    sentence = str1\n",
    "    for j in range(num_sub_tokens_label):\n",
    "        sentence = sentence + \" <mask>\"\n",
    "    inputs = tokenizer.encode_plus(sentence, add_special_tokens=True,return_tensors='pt')\n",
    "    masked_pos = []\n",
    "    for j in range(len(inputs['input_ids'][0])):\n",
    "        if inputs['input_ids'][0][j] == 50264: #103\n",
    "            masked_pos.append(j)\n",
    "    # Get the masked token position\n",
    "\n",
    "    # Generate predictions for the masked token\n",
    "    with torch.no_grad():\n",
    "        predictions = model(**inputs).logits\n",
    "\n",
    "    a = tokenizer.convert_tokens_to_ids(x)\n",
    "\n",
    "    # Retrieve the probability of the second word being the correct prediction\n",
    "    # try:\n",
    "    word2_probability = 1\n",
    "    for k in range(len(masked_pos)):\n",
    "        c = torch.softmax(predictions[0, masked_pos[k]], dim=0)[a[k]]\n",
    "        word2_probability *= c\n",
    "    return word2_probability.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a627b716",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T05:35:08.427992Z",
     "start_time": "2023-08-23T05:35:08.426270Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_word_meanings(word):\n",
    "    synsets = wordnet.synsets(word) \n",
    "    if synsets:\n",
    "        meanings = [synset.definition() for synset in synsets]\n",
    "        return meanings\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "966a5c7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T05:35:08.430694Z",
     "start_time": "2023-08-23T05:35:08.428880Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_doc(java_code):\n",
    "    lines = java_code.splitlines()\n",
    "    lines_with_mask = [line for line in lines if \"[MASK]\" in line]\n",
    "    return lines_with_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84454fb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T05:35:08.437730Z",
     "start_time": "2023-08-23T05:35:08.431675Z"
    }
   },
   "outputs": [],
   "source": [
    "def rew(inp, word, clean_text):\n",
    "    reward = torch.full((1,), fill_value=0.0)\n",
    "    w_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', word)\n",
    "    xi = torch.full((1,), fill_value=1)\n",
    "    yi = torch.full((1,), fill_value=2)\n",
    "    for p in range(len(w_list)):\n",
    "        w = w_list[p]\n",
    "        if p==0:\n",
    "            if not w.islower():\n",
    "                reward-=yi\n",
    "            continue\n",
    "        if not w[0].isupper():\n",
    "            reward-=xi\n",
    "            w[0].lower()\n",
    "        if not w.islower():\n",
    "            reward-=yi\n",
    "    comments = extract_comments(inp)\n",
    "    str1 = \"\"\n",
    "    for comment in comments:\n",
    "        str1+=comment\n",
    "        str1+=\"\\n\"\n",
    "    pred_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', word)\n",
    "    str2 = ' '.join(pred_list)\n",
    "    nlp = en_core_web_sm.load()\n",
    "    tokens1 = nlp(str1)\n",
    "    tokens2 = nlp(str2)\n",
    "    if len(tokens1) == 0 or len(tokens2) == 0:\n",
    "        reward +=0\n",
    "    else:\n",
    "        embedding1 = sum(token.vector for token in tokens1) / len(tokens1)\n",
    "        embedding2 = sum(token.vector for token in tokens2) / len(tokens2)\n",
    "        w1= LA.norm(embedding1)\n",
    "        w2= LA.norm(embedding2)\n",
    "        sim = (embedding1.dot(embedding2) / (w1 * w2))\n",
    "        sim*=10\n",
    "        simi = torch.full((1,), fill_value=sim)\n",
    "        reward+=simi\n",
    "    str1 = clean_text\n",
    "    nlp = en_core_web_sm.load()\n",
    "    tokens1 = nlp(str1)\n",
    "    tokens2 = nlp(str2)\n",
    "    if len(tokens1) == 0 or len(tokens2) == 0:\n",
    "        reward +=0\n",
    "    else:\n",
    "        embedding1 = sum(token.vector for token in tokens1) / len(tokens1)\n",
    "        embedding2 = sum(token.vector for token in tokens2) / len(tokens2)\n",
    "        w1= LA.norm(embedding1)\n",
    "        w2= LA.norm(embedding2)\n",
    "        sim = (embedding1.dot(embedding2) / (w1 * w2))\n",
    "        sim*=10\n",
    "        simi = torch.full((1,), fill_value=sim)\n",
    "        reward+=simi\n",
    "    w_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', word)\n",
    "    o = 0\n",
    "    tot_prob = 1\n",
    "    if len(w_list) > 1:\n",
    "        str1 = \"\"\n",
    "        for wordi in w_list:\n",
    "            if o == 0:\n",
    "                str1+=wordi\n",
    "                str1+=\" \"\n",
    "                o+=1\n",
    "                continue\n",
    "            tot_prob = tot_prob*calculate_co_occurrence_probability(str1,wordi)\n",
    "            str1+=wordi\n",
    "            str1+=\" \"\n",
    "    a = len(w_list)\n",
    "    tot_prob*=10**(2**a)\n",
    "    tot_prob = np.log(tot_prob)\n",
    "    di = torch.full((1,), fill_value=tot_prob)\n",
    "    reward+= di\n",
    "    w_list = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', word)\n",
    "    counter = 0\n",
    "    po = ['NN', 'FW', 'NNS']\n",
    "    for l in range(len(w_list)):\n",
    "        b = nltk.pos_tag([w_list[l]])\n",
    "        if b[0][1] in po:\n",
    "            a = find_word_meanings(w_list[l])\n",
    "            counter+= len(a)\n",
    "    counter-=len(w_list)\n",
    "    zi = torch.full((1,), fill_value=counter)\n",
    "    reward-=zi\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b5116a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T05:35:08.442373Z",
     "start_time": "2023-08-23T05:35:08.440158Z"
    }
   },
   "outputs": [],
   "source": [
    "def reward_function(inputs,model_output):\n",
    "    for i in range(len(model_output)):\n",
    "        global u\n",
    "        val = rew(inputs[0][i],model_output[i]['actual_pred'], inputs[2][i])\n",
    "        if u == 0:\n",
    "            rewards = val\n",
    "            u+=1\n",
    "        else:\n",
    "            xi = val\n",
    "            rewards = torch.cat((rewards, xi), dim=0)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "656f35a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T05:35:08.453267Z",
     "start_time": "2023-08-23T05:35:08.443512Z"
    }
   },
   "outputs": [],
   "source": [
    "# with current batch_size of 5\n",
    "u = 0\n",
    "def ppo_algorithm(transformer_model, reward_function, dataset, batch_size=1, clip_param=0.2,\n",
    "                  value_loss_coef=0.5, entropy_coef=0.01, lr=0.001, kl_coef=0.1):\n",
    "    optimizer = optim.Adam(transformer_model.parameters(), lr=lr)\n",
    "    value_criterion = nn.MSELoss()\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    global u\n",
    "    global human_input_prob\n",
    "    global dfn\n",
    "    global epoch_number\n",
    "    \n",
    "    for batch in dataset:\n",
    "        # Extract inputs and labels from the batch\n",
    "        u = 0\n",
    "        inputs = batch\n",
    "#         print(inputs)\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        # Make predictions for this batch\n",
    "        model_output = []\n",
    "        output = []\n",
    "#         try:\n",
    "        for i in range(len(inputs[0])):\n",
    "            l = []\n",
    "            l.append(inputs[0][i])\n",
    "            l.append(inputs[1][i])\n",
    "            opi = transformer_model(l)\n",
    "            model_output.append(opi)\n",
    "            output.append(opi['actual_pred'])\n",
    "#         except:\n",
    "#             continue\n",
    "        # Calculate rewards\n",
    "        rewardsi = reward_function(inputs, model_output)\n",
    "        # Calculate value loss\n",
    "        for i in range(len(model_output)):\n",
    "            if i == 0:\n",
    "                values = model_output[i]['logits']\n",
    "                old_values =  model_output[i]['old_logits']\n",
    "                mhs = model_output[i]['mhs']\n",
    "                old_mhs = model_output[i]['old_mhs']\n",
    "                log_probs = model_output[i]['log_probs']\n",
    "                old_log_probs = model_output[i]['old_log_probs']\n",
    "            elif i == 1:\n",
    "                xj = model_output[i]['logits']\n",
    "                yj = model_output[i]['old_logits']\n",
    "                zj = model_output[i]['mhs']\n",
    "                dj = model_output[i]['old_mhs']\n",
    "                aj = model_output[i]['log_probs']\n",
    "                qj = model_output[i]['old_log_probs']\n",
    "                values = torch.stack((values, xj), dim=0)\n",
    "                old_values = torch.stack((old_values, yj), dim=0)\n",
    "                mhs = torch.stack((mhs, zj), dim=0)\n",
    "                old_mhs = torch.stack((old_mhs, dj), dim=0)\n",
    "                log_probs = torch.stack((log_probs, aj), dim=0)\n",
    "                old_log_probs = torch.stack((old_log_probs, qj), dim=0)\n",
    "            else:\n",
    "                xj = model_output[i]['logits']\n",
    "                xj = xj.unsqueeze(0)\n",
    "                xj = xj.expand(1,-1,-1)\n",
    "                yj = model_output[i]['old_logits']\n",
    "                yj = yj.unsqueeze(0)\n",
    "                yj = yj.expand(1,-1,-1)\n",
    "                zj = model_output[i]['mhs']\n",
    "                zj = zj.unsqueeze(0)\n",
    "                zj = zj.expand(1,-1)\n",
    "                dj = model_output[i]['old_mhs']\n",
    "                dj = dj.unsqueeze(0)\n",
    "                dj = dj.expand(1,-1)\n",
    "                aj = model_output[i]['log_probs']\n",
    "                aj = aj.unsqueeze(0)\n",
    "                aj = aj.expand(1,-1,-1)\n",
    "                qj = model_output[i]['old_log_probs']\n",
    "                qj = qj.unsqueeze(0)\n",
    "                qj = qj.expand(1,-1,-1)\n",
    "                values = torch.cat((values, xj), dim=0)\n",
    "                old_values = torch.cat((old_values, yj), dim=0)\n",
    "                mhs = torch.cat((mhs, zj), dim=0)\n",
    "                old_mhs = torch.cat((old_mhs, dj), dim=0)\n",
    "                log_probs = torch.cat((log_probs, aj), dim=0)\n",
    "                old_log_probs = torch.cat((old_log_probs, qj), dim=0)\n",
    "        vj = []\n",
    "        cj = []\n",
    "        for j in range(values.size()[0]):\n",
    "            vj.append(value_criterion(old_values[j],values[j]))\n",
    "            cj.append(value_criterion(old_mhs[j],mhs[j]).exp())\n",
    "#         print(vj)\n",
    "        ratios = torch.stack(cj, dim=0)\n",
    "        value_loss = torch.stack(vj, dim=0)\n",
    "        entropy_loss = -(log_probs * log_probs.exp()).sum(dim=-1).mean()\n",
    "        h = random.random()\n",
    "        kl_divergence = (old_log_probs.exp() * (old_log_probs - log_probs)).mean()\n",
    "        if epoch_number < 2:\n",
    "            h = 2\n",
    "        if h < human_input_prob:\n",
    "            t['inputs'].append(list(inputs[0]))\n",
    "            t['y'].append(list(inputs[1]))\n",
    "            t['output'].append(output)\n",
    "            t['calc_reward'].append(rewardsi)\n",
    "            t['entropy_loss'].append(entropy_loss)\n",
    "            t['value_loss'].append(value_loss)\n",
    "            t['ratios'].append(ratios)\n",
    "            t['kl'].append(kl_divergence)\n",
    "#             tdf = pd.DataFrame(t)\n",
    "#             dfn = pd.concat([dfn,tdf])\n",
    "            continue\n",
    "        baseline = rewardsi.detach().mean()\n",
    "        advantages = rewardsi - baseline\n",
    "        value_loss *= advantages\n",
    "        # Calculate action loss\n",
    "        surrogate_loss = torch.min(ratios * advantages, torch.clamp(ratios, 1 - clip_param, 1 + clip_param) * advantages)\n",
    "        action_loss = -surrogate_loss.mean()\n",
    "        \n",
    "        # Calculate total loss\n",
    "        total_loss = value_loss_coef * value_loss + action_loss - entropy_coef * entropy_loss + kl_coef * kl_divergence\n",
    "        # Backward pass and optimization step\n",
    "        print(\"hurt\")\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.sum().backward()\n",
    "        optimizer.step()\n",
    "        running_loss += total_loss.item()\n",
    "        if i % 1 == 0:\n",
    "            last_loss = running_loss / 5 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "#             tb_writer.add_scalar('Loss1/train1', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "    return transformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abee3f63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T05:35:08.460306Z",
     "start_time": "2023-08-23T05:35:08.454465Z"
    }
   },
   "outputs": [],
   "source": [
    "# with current batch_size of 5\n",
    "def human_ppo(transformer_model, clip_param=0.2, value_loss_coef=0.5, entropy_coef=0.01, lr=0.001, kl_coef = 0.1):\n",
    "    optimizer = optim.Adam(transformer_model.parameters(), lr=lr)\n",
    "    value_criterion = nn.MSELoss()\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    global t\n",
    "    global epoch_number\n",
    "    global run_int\n",
    "    m1 =  Step1_model()\n",
    "    m1.load_state_dict(torch.load('var_runs/model_{}_{}'.format(run_int,epoch_number-1)))\n",
    "    m2 =  Step1_model()\n",
    "    m2.load_state_dict(torch.load('var_runs/model_{}_{}'.format(run_int,epoch_number-2)))\n",
    "    for r in range(len(t['inputs'])):\n",
    "        print(\"b\")\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        # Make predictions for this batch\n",
    "        inputs = t['inputs'][r]\n",
    "        y_list = t['y'][r]\n",
    "        entropy_loss = t['entropy_loss'][r]\n",
    "        ratios = t['ratios'][r]\n",
    "        value_loss = t['value_loss'][r]\n",
    "        rewardsi = t['calc_reward'][r]\n",
    "        output = t['output'][r]\n",
    "        kl_divergence = t['kl'][r]\n",
    "#         inputs = dfn.iloc[r]['examples']\n",
    "#         y_list = dfn.iloc[r]['y']\n",
    "#         entropy_loss = dfn.iloc[r]['entropy_loss']\n",
    "#         value_loss = dfn.iloc[r]['value_loss']\n",
    "#         ratios = dfn.iloc[r]['ratios']\n",
    "#         output = dfn.iloc[r]['output']\n",
    "#         rewards = dfn.iloc[r]['calc_rew']\n",
    "        for q in range(len(inputs)):\n",
    "            ans_list = []\n",
    "            ans_list.append(output[q])\n",
    "            l = []\n",
    "            l.append(inputs[q])\n",
    "            l.append(y_list[q])\n",
    "            o = m1(l)\n",
    "            o1 = m2(l)\n",
    "            ans_list.append(o['actual_pred'])\n",
    "            ans_list.append(o1['actual_pred'])\n",
    "            print(\"Here is the code with masked variable name:\")\n",
    "            print(inputs[q])\n",
    "            print(\"Which of the answers is better?\")\n",
    "            print(\"A. \"+str(ans_list[0]))\n",
    "            print(\"B. \"+str(ans_list[1]))\n",
    "            print(\"C. \"+str(ans_list[2]))\n",
    "            print(\"D. \"+str(y_list[q]))\n",
    "            ans = input(\"Type a, b, c, or d\")\n",
    "            if ans.lower() == 'a':\n",
    "                rewardsi[q] += 200\n",
    "            elif ans.lower() == 'b':\n",
    "                rewardsi[q] -= 100\n",
    "            elif ans.lower() == 'c':\n",
    "                rewardsi[q] -= 200\n",
    "            elif ans.lower() == 'd':\n",
    "                rewardsi[q] -= 400\n",
    "            else:\n",
    "                print(\"error, no choice made\")\n",
    "        baseline = rewardsi.detach().mean()\n",
    "        advantages = rewardsi - baseline\n",
    "        value_loss *= advantages\n",
    "        # Calculate action loss\n",
    "        surrogate_loss = torch.min(ratios * advantages, torch.clamp(ratios, 1 - clip_param, 1 + clip_param) * advantages)\n",
    "        action_loss = -surrogate_loss.mean()\n",
    "\n",
    "        # Calculate entropy loss\n",
    "\n",
    "        # Calculate total loss\n",
    "        total_loss = value_loss_coef * value_loss + action_loss - entropy_coef * entropy_loss + kl_coef * kl_divergence\n",
    "        # Backward pass and optimization step\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.sum().backward()\n",
    "        optimizer.step()\n",
    "        running_loss += total_loss.item()\n",
    "#         if r % 1 == 0:\n",
    "#             last_loss = running_loss / 32 # loss per batch\n",
    "#             print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "#             tb_x = epoch_index * len(train_loader) + i + 1\n",
    "# #             tb_writer.add_scalar('Loss1/train1', last_loss, tb_x)\n",
    "#             running_loss = 0.\n",
    "    return transformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ebf7c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-23T05:35:05.362Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10878 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "yTrans\n",
      "\n",
      "0\n",
      "number of mask tok 8\n",
      "number of seq 4\n",
      "1\n",
      "number of mask tok 4\n",
      "number of seq 4\n",
      "Guess :  yTrans\n",
      "one\n",
      "missed\n",
      "\n",
      "0\n",
      "number of mask tok 8\n",
      "number of seq 4\n",
      "1\n",
      "number of mask tok 4\n",
      "number of seq 4\n",
      "Guess :  active\n",
      "hurt\n"
     ]
    }
   ],
   "source": [
    "human_input_prob = 0.02\n",
    "epoch_number = 0\n",
    "EPOCHS = 10\n",
    "run_int = 0\n",
    "model = Step1_model()\n",
    "# model = torch.load()\n",
    "model = freeze(model)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "myDs=MyDataset('dat.csv')\n",
    "t = {'inputs':[],'y':[],'output':[],'calc_reward':[], 'ratios':[],\n",
    "                'entropy_loss':[],'value_loss':[], 'kl':[]}\n",
    "train_loader=DataLoader(myDs,batch_size=2,shuffle=True)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    model = ppo_algorithm(model, reward_function, train_loader)\n",
    "    if epoch >= 2:\n",
    "        model = human_ppo(model)\n",
    "    for key in t.keys():\n",
    "        t[key].clear()\n",
    "    model_path = 'rl_runs/model_{}_{}'.format(run_int, epoch_number)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to get tokenizer and model for this, generate summary of lines with <mask>,\n",
    "# and for lines between first and last <mask>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc7448f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflown)",
   "language": "python",
   "name": "tensorflown"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
