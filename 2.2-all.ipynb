{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90966178",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:42:15.633193Z",
     "start_time": "2023-09-28T12:42:09.694324Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init, MarginRankingLoss\n",
    "from transformers import BertModel, RobertaModel\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "from torch.optim import Adam\n",
    "from distutils.version import LooseVersion\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "import nltk\n",
    "import re\n",
    "import Levenshtein\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.distributions import Categorical\n",
    "from numpy import linalg as LA\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from nltk.corpus import wordnet\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from nltk.corpus import words as wal\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "209f54a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:42:15.636834Z",
     "start_time": "2023-09-28T12:42:15.634671Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,file_name):\n",
    "        df1 = pd.read_csv(file_name)\n",
    "        df1 = df1.fillna(\"\")\n",
    "        res = df1['X']\n",
    "        self.X_list = res\n",
    "        self.y_list = df1['y']\n",
    "    def __len__(self):\n",
    "        return len(self.X_list)\n",
    "    def __getitem__(self,idx):\n",
    "        mapi = []\n",
    "        mapi.append(self.X_list[idx])\n",
    "        mapi.append(self.y_list[idx])\n",
    "        return mapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98bf7764",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T12:42:18.645418Z",
     "start_time": "2023-09-28T12:42:15.637791Z"
    }
   },
   "outputs": [],
   "source": [
    "epoch_number = 0\n",
    "EPOCHS = 5\n",
    "run_int = 26\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-3)\n",
    "myDs=MyDataset('dat1.csv') # 10-20% of num_training_steps\n",
    "model.train()\n",
    "train_loader=DataLoader(myDs,batch_size=24,shuffle=True)\n",
    "best_loss = torch.full((1,), fill_value=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f38e10",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-28T12:42:42.759Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                   | 0/209 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attributeNamesToIgnore\n",
      "\n",
      "number of mask tok 16\n",
      "number of seq 4\n",
      "AttributeAttributeNames\n",
      "*****\n",
      "newIndex\n",
      "\n",
      "number of mask tok 8\n",
      "number of seq 4\n",
      "StartIndex\n",
      "*****\n",
      "requestForResponse\n",
      "\n",
      "number of mask tok 9\n",
      "number of seq 3\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    tot_loss = 0.0\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = batch\n",
    "        maxi = torch.tensor(0.0, requires_grad=True)\n",
    "        for i in range(len(batch[0])):\n",
    "            X_init = inputs[0][i]\n",
    "            y = inputs[1][i]\n",
    "            ty = tokenizer.encode(y)[1:-1]\n",
    "            print(y)\n",
    "            nl = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))|[a-z]+|\\d+', y)\n",
    "            lb = ' '.join(nl).lower()\n",
    "            x = tokenizer.tokenize(lb)\n",
    "            num_sub_tokens_label = len(x)\n",
    "            X_init = X_init.replace(\"[MASK]\", \" \".join([tokenizer.mask_token] * num_sub_tokens_label))\n",
    "            preds = []\n",
    "            tokens = tokenizer.encode_plus(X_init, add_special_tokens=False,return_tensors='pt')\n",
    "            input_id_chunki = tokens['input_ids'][0].split(510)\n",
    "            input_id_chunks = []\n",
    "            mask_chunks  = []\n",
    "            mask_chunki = tokens['attention_mask'][0].split(510)\n",
    "            for tensor in input_id_chunki:\n",
    "                input_id_chunks.append(tensor)\n",
    "            for tensor in mask_chunki:\n",
    "                mask_chunks.append(tensor)\n",
    "            xi = torch.full((1,), fill_value=101)\n",
    "            yi = torch.full((1,), fill_value=1)\n",
    "            zi = torch.full((1,), fill_value=102)\n",
    "            for r in range(len(input_id_chunks)):\n",
    "                input_id_chunks[r] = torch.cat([xi, input_id_chunks[r]],dim = -1)\n",
    "                input_id_chunks[r] = torch.cat([input_id_chunks[r],zi],dim=-1)\n",
    "                mask_chunks[r] = torch.cat([yi, mask_chunks[r]],dim=-1)\n",
    "                mask_chunks[r] = torch.cat([mask_chunks[r],yi],dim=-1)\n",
    "            di = torch.full((1,), fill_value=0)\n",
    "            for i in range(len(input_id_chunks)):\n",
    "                # get required padding length\n",
    "                pad_len = 512 - input_id_chunks[i].shape[0]\n",
    "                # check if tensor length satisfies required chunk size\n",
    "                if pad_len > 0:\n",
    "                    # if padding length is more than 0, we must add padding\n",
    "                    for p in range(pad_len):\n",
    "                        input_id_chunks[i] = torch.cat([input_id_chunks[i],di],dim=-1)\n",
    "                        mask_chunks[i] = torch.cat([mask_chunks[i],di],dim=-1)\n",
    "            input_ids = torch.stack(input_id_chunks)\n",
    "            attention_mask = torch.stack(mask_chunks)\n",
    "            maski = []\n",
    "            u = 0\n",
    "            ad = 0\n",
    "            labels = input_ids.clone()\n",
    "            for l in range(len(input_ids)):\n",
    "                masked_pos = []\n",
    "                for i in range(len(input_ids[l])):\n",
    "                    if input_ids[l][i] == 50264: #103\n",
    "                        u+=1\n",
    "                        if i != 0 and input_ids[l][i-1] == 50264:\n",
    "                            continue\n",
    "                        for t in range(num_sub_tokens_label):\n",
    "                            if (i+t) > len(labels[l]):\n",
    "                                labels[l+1][i+t-len(labels[l])] = ty[t]\n",
    "                            else:\n",
    "                                labels[l][i+t] = ty[t]\n",
    "                        masked_pos.append(i)\n",
    "                        ad+=1\n",
    "                    else:\n",
    "                        labels[l][i] = -100\n",
    "                maski.append(masked_pos)\n",
    "            input_dict = {\n",
    "                'input_ids': input_ids.long(),\n",
    "                'attention_mask': attention_mask.int()\n",
    "            }\n",
    "            print('number of mask tok',u)\n",
    "            print('number of seq', ad)\n",
    "            outputs = model(input_dict['input_ids'], attention_mask = input_dict['attention_mask'])\n",
    "            last_hidden_state = outputs[0].squeeze()\n",
    "            l_o_l_sa = []\n",
    "            sum_state = []\n",
    "            for t in range(num_sub_tokens_label):\n",
    "                c = []\n",
    "                l_o_l_sa.append(c)\n",
    "            if len(maski) == 1:\n",
    "                masked_pos = maski[0]\n",
    "                for k in masked_pos:\n",
    "                    for t in range(num_sub_tokens_label):\n",
    "                        l_o_l_sa[t].append(last_hidden_state[k+t])\n",
    "            else:\n",
    "                for p in range(len(maski)):\n",
    "                    masked_pos = maski[p]\n",
    "                    for k in masked_pos:\n",
    "                        for t in range(num_sub_tokens_label):\n",
    "                            if (k+t) > len(last_hidden_state[p]):\n",
    "                                l_o_l_sa[t].append(last_hidden_state[p+1][k+t-len(last_hidden_state[p])])\n",
    "                                continue\n",
    "                            l_o_l_sa[t].append(last_hidden_state[p][k+t])\n",
    "            for t in range(num_sub_tokens_label):\n",
    "                sum_state.append(l_o_l_sa[t][0])\n",
    "            for i in range(len(l_o_l_sa[0])):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                for t in range(num_sub_tokens_label):\n",
    "                    sum_state[t] = sum_state[t] + l_o_l_sa[t][i]\n",
    "            yip = len(l_o_l_sa[0])\n",
    "            qw = \"\"\n",
    "            val = torch.tensor(0.0, requires_grad=True)\n",
    "            for t in range(num_sub_tokens_label):\n",
    "                sum_state[t] /= yip\n",
    "                probs = F.softmax(sum_state[t], dim=0)\n",
    "                val = val - torch.log(probs[ty[t]])\n",
    "                idx = torch.topk(sum_state[t], k=1, dim=0)[1]\n",
    "                qw += [tokenizer.decode(i.item()).strip() for i in idx][0].capitalize()\n",
    "            val = val / num_sub_tokens_label\n",
    "            maxi = maxi + val\n",
    "            print(qw)\n",
    "            print(\"*****\")\n",
    "        tot_loss +=maxi\n",
    "        maxi = maxi / len(batch[0])\n",
    "        maxi.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        print(maxi)\n",
    "        loop.set_postfix(loss=maxi.item())\n",
    "    tot_loss/=len(myDs)\n",
    "    print(tot_loss)\n",
    "    if tot_loss < best_loss:\n",
    "        best_loss = tot_loss\n",
    "    model_path = 'var_runs/model_{}_{}'.format(run_int, epoch)\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caef03d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflown)",
   "language": "python",
   "name": "tensorflown"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
